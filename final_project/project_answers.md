## Enron Submission Free-Response Answers

1. The goal of this project was to indentify workers at Enron who may have committed fraud through the financial and email data of the corporation. The purpose was to flag possible persons of interest for further investigation. The data used was made up of 146 datapoints with 19 persons of interest meaning 13% of persons were class labeled as POI. Through data exploration, I was able to identify one outlier in the financial data - TOTAL. This data point was probably included in the dataset from a spreadsheet program and was degrees of magnitude higher than any other data point. I handled it by removing it in my data cleansing step from the data by popping it out of the dictionary that contained the data points. I also discovered there were some features with missing values listed as NaN.

2. I started with a total of 9 features which I then fed into a Pipeline containing Principal Component Analysis (PCA), which fitted and transformed the data reducing it to five features with the most variance. When I created two new features - bonus_salary_ratio and total_stock_total_payment_ratio, I used the MinMaxScaler class from sklearn to scale my input features, bonus, salary, total_stock_value, total_payments to be between 0 and 1. The created features were ratios I believed could encapsulate greater insights as relates to the data. The first one was the ratio of bonus to salary and I reasoned that a bigger ratio could indicate a larger bonus, as this person had access to more deals where fraud could probably be perpetuated. The second was a ratio of total stock value to total payments and I felt an indication of greater stock holdings could mean the person had more access to the complex financial instruments which was used to commit fraud at Enron. Both hypothesis stood up to scruitny but did not greatly influence my metrics when measured on the test data as I had slightly similar results as before. The feature importances of my base DecisionTreeClassifier was [ 0.07215007  0.09097554  0.47066354  0.04084177  0.32536908] for the five features used.

3. I ended up using an ensemble classifier - AdaBoostClassifier. The reason for this was because my DecisionTreeClassifier outperformed other models by few percentage points. The KNearest Neighbor classifier and Gaussian Naive Bayes classifier were the next best performing algorithms. My decision to use the AdaBoostClassifier with the DecisionTreeClassifier as the base estimator improved my evaluation metrics.

4. Tunning the parameters of an algorithm means to adjust the settings knob of a classifier. Proper parameter tunning in line with the dataset under investigation can lead to improved performance. However, when this is applied wrongly, especially when more complexity than is necessary is added, this can lead to overfitting of data which simply means an algorithm is able to perform strongly on training data but performs poorly on test or real world data because it does not generalise sufficiently. I tunned my AdaBoostClassifier by setting DecisionTreeClassifier as the base estmator and fixing the maximum number of estimators at which boosting is terminated to 50.

5. Validation can simply be seen as the process of testing the integrity of the predictions of your model on some data. A classic mistake of validation is running tests on the same set of data that was used to train the model. This would lead to high confidence scores because the model is basically making predictions on data that it is already familiar with. Such an approach would lead to disastrous results in production when the model is tested againsted real world data. 
The recommended approach is to split your data into training and testing sets respectively. The first should be used to train the model while the second should be used to validate the model. In this project, I tried two methods, I used the train_test_split function and KFold class from sklearn. I ultimately settled on train_test_split with a test size of 0.3 or 30% mainly because of the limited size of the data. KFold validation didn't produce better results as the number of POIs in each fold was greatly reduced.

6. The evaluation metrics I considered were precision, recall and accuracy. The average performance was 0.32109, 0.33650 and 0.81667 respectively when using tester.py and 0.75, 0.5, 0.907 when using the test set. The higher the precision, the less number of false positives were found which means that the likelihood of a person being wrongly predicted as a POI when he/she was not was highly reduced. For a higher recall score, this indicated that there were fewer false negatives, that is the likehood of a POI being predicted as a non POI.